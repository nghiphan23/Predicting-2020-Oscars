{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Having collected, cleaned and organized the data with information about ratings, cast, genre and past Oscar performances, we will now try to predict the winners and nominees. The approach that we take is the following. \n",
    "\n",
    "Because we have seen that a movie almost has to be in one of the three major categories -- drama, romance, comedy or biograpjy -- we will filter our data to only consider the movie that fall into oine of thes egenres. \n",
    "We will then use a variety of different models to predict the probabibility of win for the different movies. The movies will be sorted in descending order of win probabilities. \n",
    "\n",
    "Finally, we will take a aweighted average of al the prediction, weighted by the recall score, and use the grand average as our final prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "import requests\n",
    "import wikipedia\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from scrapy import selector\n",
    "import datetime as dt\n",
    "import pickle\n",
    "from skimage import io\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "# scikit learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oscar genres:\n",
      " ['sci-fi', 'family', 'adventure', 'crime', 'biography', 'history', 'musical', 'romance', 'sport', 'comedy', 'horror', 'drama', 'mystery', 'war', 'thriller']\n",
      "Oscar categories:\n",
      " ['picture', 'director', 's_actor', 's_actress', 'actor', 'actress', 'screenplay']\n",
      "Predictor columns:\n",
      " ['cast_size', 'genre_span', 'sci-fi', 'family', 'adventure', 'crime', 'biography', 'history', 'musical', 'romance', 'sport', 'comedy', 'horror', 'drama', 'mystery', 'war', 'thriller', 'n_votes', 'imdb_rating', 'metscore', 'rotten_tomatoes', 'precount_wins', 'precount_noms', 'other_wins', 'other_noms']\n"
     ]
    }
   ],
   "source": [
    "DF_main = pd.read_csv('my_data/df_main_FINAL.csv', index_col=[0])\n",
    "all_genres = pickle.load(open(\"my_data/all_genres_FINAL\",\"rb\"))\n",
    "oscar_categories = pickle.load(open(\"my_data/major_oscar_categories\",\"rb\"))\n",
    "oscar_genres_columns = ['sci-fi', 'family', 'adventure', 'crime', \n",
    "                'biography', 'history', 'musical', 'romance', 'sport', 'comedy', \n",
    "                'horror', 'drama', 'mystery', 'war', 'thriller']\n",
    "\n",
    "\n",
    "\n",
    "ID_columns = ['imdbID', 'title', 'year']\n",
    "basic_columns = ['cast_size', 'genre_span']\n",
    "\n",
    "    \n",
    "print(\"Oscar genres:\\n\",oscar_genres_columns)\n",
    "print(\"Oscar categories:\\n\", oscar_categories)\n",
    "\n",
    "\n",
    "\n",
    "scores = ['n_votes','imdb_rating', 'metscore', 'rotten_tomatoes']\n",
    "awards_pre = ['precount_wins', 'precount_noms', 'other_wins', 'other_noms']\n",
    "awards_post = ['win', 'nom']\n",
    "\n",
    "predictor_columns = basic_columns + oscar_genres_columns + scores + awards_pre\n",
    "\n",
    "print(\"Predictor columns:\\n\",predictor_columns)\n",
    "target_columns = ['win', 'nom']\n",
    "filters = ((DF_main.drama==1)|((DF_main.comedy==1)|(DF_main.romance==1)|(DF_main.biography==1)))&(DF_main.action==0)\n",
    "DF_main = DF_main[filters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 2019 movies are being considered:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(DF_main.query(\"year == 2019\"))} 2019 movies are being considered:\\n')\n",
    "# print(list(DF_main.query(\"year == 2019\").title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pctile(X, columns):\n",
    "    \n",
    "    for col in columns:\n",
    "        x = np.array(X[col])\n",
    "        X[col] = [(len(np.where(x<=y)[0])/len(x)) for y in x]\n",
    "    return X\n",
    "\n",
    "def normalize_by_max(X, columns):\n",
    "    for col in columns:\n",
    "        x = np.array(X[col])\n",
    "        mx = np.max(x)\n",
    "        X[col] = x/mx\n",
    "    return X\n",
    "\n",
    "def top_N_each_year(df,N,feature):\n",
    "    df_ = pd.DataFrame()\n",
    "    for year in list(set(df.year)):\n",
    "        # print(year)\n",
    "        df_ = df_.append(df[df.year == year].sort_values(by = feature, ascending=False).head(N))\n",
    "    return df_\n",
    "\n",
    "def normalize_by_year(df, columns, _how = 'max'):\n",
    "    \n",
    "    if type(columns)!= list:\n",
    "        print(\"columns must be list\")\n",
    "        return dict()\n",
    "    years = list(set(df.year))\n",
    "\n",
    "    print(years)\n",
    "    \n",
    "    if _how == 'pctile':\n",
    "        df = convert_to_pctile(df, columns)\n",
    "        return df\n",
    "\n",
    "    for column in columns:\n",
    "        cols = ['year'] + [column]\n",
    "        for year in years:\n",
    "            if year%10 == 0:\n",
    "                clear_output() \n",
    "            print(year, column)\n",
    "            temp_df = df[(df.year == year)]\n",
    "            temp_df = temp_df[column]\n",
    "            ids = temp_df.index\n",
    "            \n",
    "            if _how == 'max':\n",
    "                df.loc[ids,column] = df.loc[ids,column]/temp_df.max()\n",
    "            if _how == 'minmax':\n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                vals = df.loc[ids,column].values\n",
    "                # vals.shape = (len(vals),1)\n",
    "                print(vals.shape)\n",
    "                vals = vals[:,np.newaxis]\n",
    "                print(vals.shape)\n",
    "                scaled_array = min_max_scaler.fit_transform(vals)\n",
    "                df.loc[ids,column] = scaled_array\n",
    "            \n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cast_size', 'genre_span', 'sci-fi', 'family', 'adventure', 'crime', 'biography', 'history', 'musical', 'romance', 'sport', 'comedy', 'horror', 'drama', 'mystery', 'war', 'thriller', 'n_votes', 'imdb_rating', 'metscore', 'rotten_tomatoes', 'precount_wins', 'precount_noms', 'other_wins', 'other_noms']\n"
     ]
    }
   ],
   "source": [
    "print(predictor_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 other_noms\n",
      "2011 other_noms\n",
      "2012 other_noms\n",
      "2013 other_noms\n",
      "2014 other_noms\n",
      "2015 other_noms\n",
      "2016 other_noms\n",
      "2017 other_noms\n",
      "2018 other_noms\n",
      "2019 other_noms\n"
     ]
    }
   ],
   "source": [
    "print(DF_main.columns)\n",
    "normalize_columns = ['n_votes', 'imdb_rating', 'metscore', 'rotten_tomatoes',\n",
    "                     'cast_size', 'genre_span', 'cast_size', 'running_time']\n",
    "DF = normalize_by_year(DF_main, predictor_columns, 'max')\n",
    "DF.fillna('0',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF = DF[(DF.year >=1960)&(DF.year < 2019)]\n",
    "# DFX = DF.loc[:,ID_columns + predictor_columns + target_columns]\n",
    "# print(ID_columns + predictor_columns + target_columns)\n",
    "# DFX.columns\n",
    "# DFX.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_2019 shape: (192, 25)\n"
     ]
    }
   ],
   "source": [
    "DFX_2019 = DF[DF.year == 2019]\n",
    "DFX_2019 = DFX_2019.loc[:,ID_columns + predictor_columns + target_columns]\n",
    "X_2019 = DFX_2019[predictor_columns]\n",
    "print(\"X_2019 shape:\",X_2019.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8981, 25)\n",
      "y shape: (8981, 1)\n",
      "X_2019 shape: (192, 25)\n",
      "Class 0: 6082\n",
      "Class 1: 204\n",
      "2019:\n"
     ]
    }
   ],
   "source": [
    "# Pre 2019 (predictor) movies\n",
    "\n",
    "# Predictor matrix\n",
    "DF1 = DF[(DF.year >=1960)&(DF.year < 2019)]\n",
    "DFX = DF1.loc[:,ID_columns + predictor_columns + target_columns]\n",
    "X = DFX[predictor_columns]\n",
    "print(\"X shape:\",X.shape)\n",
    "\n",
    "\n",
    "#Target matrix\n",
    "awards_map = {'W':1, 'N':1, 'WN':1, 'O':0}\n",
    "category = 'picture'\n",
    "y = DF1[category].map(awards_map).values\n",
    "y = y[:,np.newaxis]\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "\n",
    "# y = (DFX.win > 1)|(DFX.nom>3) # 2 wins of 4 nominations\n",
    "# y = np.array([int(z) for z in y])\n",
    "# DFX['target'] = y\n",
    "# y = y[:,np.newaxis]\n",
    "# print(\"y shape:\", y.shape)\n",
    "\n",
    "# Now for 2019 (target) movies\n",
    "DFX_2019 = DF[DF.year == 2019]\n",
    "DFX_2019 = DFX_2019.loc[:,ID_columns + predictor_columns + target_columns]\n",
    "X_2019 = DFX_2019[predictor_columns]\n",
    "print(\"X_2019 shape:\",X_2019.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify = y)\n",
    "print(\"Class 0:\", len(y_train[np.where(y_train == 0)]))\n",
    "print(\"Class 1:\", len(y_train[np.where(y_train == 1)]))\n",
    "print(\"2019:\", )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8981 entries, 0 to 13933\n",
      "Data columns (total 25 columns):\n",
      "cast_size          8981 non-null float64\n",
      "genre_span         8981 non-null float64\n",
      "sci-fi             8981 non-null object\n",
      "family             8981 non-null object\n",
      "adventure          8981 non-null float64\n",
      "crime              8981 non-null object\n",
      "biography          8981 non-null float64\n",
      "history            8981 non-null object\n",
      "musical            8981 non-null float64\n",
      "romance            8981 non-null float64\n",
      "sport              8981 non-null object\n",
      "comedy             8981 non-null float64\n",
      "horror             8981 non-null object\n",
      "drama              8981 non-null float64\n",
      "mystery            8981 non-null object\n",
      "war                8981 non-null object\n",
      "thriller           8981 non-null object\n",
      "n_votes            8981 non-null float64\n",
      "imdb_rating        8981 non-null float64\n",
      "metscore           8981 non-null float64\n",
      "rotten_tomatoes    8981 non-null float64\n",
      "precount_wins      8981 non-null float64\n",
      "precount_noms      8981 non-null float64\n",
      "other_wins         8981 non-null float64\n",
      "other_noms         8981 non-null float64\n",
      "dtypes: float64(16), object(9)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(classifier_dict, random_state = 47):\n",
    "    \n",
    "    name = classifier_dict['classifier']\n",
    "    model = classifier_dict['classifier'](random_state = random_state)\n",
    "    if 'hyperparameters' in classifier_dict:\n",
    "        hyperparams = classifier_dict['hyperparameters']['params']\n",
    "        cv = classifier_dict['hyperparameters']['cv']\n",
    "        model = GridSearchCV(model, hyperparameters, cv=cv, verbose=0)\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=29, solver='warn',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "       5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "       3.59381366e+03, 1.00000000e+04]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier definition for logistic regression\n",
    "classifier_dict = {'name':'LogisticRegression'}\n",
    "# classifier_dict['name'] = 'LogisticRegression'\n",
    "classifier_dict['classifier'] = LogisticRegression\n",
    "\n",
    "# Hyperparameters\n",
    "classifier_dict['hyperparameters'] = {}\n",
    "penalty = ['l1', 'l2']\n",
    "C = np.logspace(0, 4, 10)\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "cv = 5\n",
    "classifier_dict['hyperparameters']['params'] = hyperparameters\n",
    "classifier_dict['hyperparameters']['cv'] = cv\n",
    "# Penalty\n",
    "classifier_dict['penalty'] = ['l1', 'l2']\n",
    "\n",
    "classifier(classifier_dict, random_state = 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1bdfdb9b67f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'class 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'class 1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "model = classifier(classifier_dict, random_state = 29)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "target_names = ['class 0', 'class 1']\n",
    "clear_output() \n",
    "x = x.iloc[::-1,:]\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "accuracy = []\n",
    "recall = []\n",
    "precision = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Because we are interested in obtaining probabilities for each film earning a nomination, and inferring the winner from these probabilities, the only linear model that was considered was Logistic Regression. Because Logistic regression uses the logistic function to model a binary dependent variable, the output of the model can be naturally interpreted as the probability of nomination or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(random_state=0)\n",
    "penalty = ['l1', 'l2']\n",
    "C = np.logspace(0, 4, 10)\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "log_cv = GridSearchCV(log, hyperparameters, cv=5, verbose=0)\n",
    "log_cv.fit(X, y)\n",
    "y_pred = log_cv.predict(X_2019)\n",
    "y_prob = log_cv.predict_proba(X_2019)[:,1]\n",
    "DFX_2019.loc[:,'predicted_probability_log'] = y_prob\n",
    "x = DFX_2019.sort_values(by='predicted_probability_log', ascending=False).head(10)\n",
    "clear_output()\n",
    "\n",
    "### Performance evaluation\n",
    "log_cv.fit(X_train, y_train)\n",
    "y_pred = log_cv.predict(X_test)\n",
    "target_names = ['class 0', 'class 1']\n",
    "clear_output() \n",
    "x = x.iloc[::-1,:]\n",
    "print(log_cv.best_params_)\n",
    "\n",
    "plt.barh(x['title'], x['predicted_probability_log'])\n",
    "plt.xlabel('probability')\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "acc_score = log_cv.score(X, y)\n",
    "prec_score = precision_score(y_test, y_pred, labels= 'class 1')\n",
    "print(\"Accuracy:\", acc_score)\n",
    "print(\"Precision:\",prec_score)\n",
    "name.append('logreg')\n",
    "precision.append(prec_score)\n",
    "accuracy.append(acc_score)\n",
    "print(name, accuracy, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name.append('log')\n",
    "# accuracy.append(0.96)\n",
    "# recall.append(0.25)\n",
    "# precision.append(0.58)\n",
    "# print(name, accuracy, recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest neighbors classifier \n",
    "K nearest neighbor is a nonlinear classifier that provided straightforward approach to classify movies based on their proximity to previous winners and nominees. We used GridSearchCV() method from the utils package of scikit-learn module. Although several variations were available, we only performed grid search on the number parameters, whose optimal value was found to be 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KNN Training\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "param_grid = {'n_neighbors': np.arange(1, 30)} \n",
    "knn_cv = GridSearchCV(knn, param_grid, cv=5) \n",
    "# knn.fit(X, y) \n",
    "knn_cv.fit(X, y)\n",
    "\n",
    "# KNN prediction\n",
    "y_pred = knn_cv.predict(X_2019)\n",
    "y_prob = knn_cv.predict_proba(X_2019)\n",
    "DFX_2019.loc[:,'predicted_probability_knn'] = y_prob[:,1]\n",
    "x = DFX_2019.sort_values(by='predicted_probability_knn', ascending=False).head(10)\n",
    "clear_output()\n",
    "\n",
    "\n",
    "### Performance evaluation\n",
    "knn_cv.fit(X_train, y_train)\n",
    "y_pred = knn_cv.predict(X_test)\n",
    "target_names = ['class 0', 'class 1']\n",
    "clear_output() \n",
    "x = x.iloc[::-1,:]\n",
    "print(knn_cv.best_params_)\n",
    "print(knn_cv.score(X, y))\n",
    "plt.barh(x['title'], x['predicted_probability_knn'])\n",
    "plt.xlabel('probability')\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "acc_score = knn_cv.score(X, y)\n",
    "prec_score = precision_score(y_test, y_pred, labels= 'class 1')\n",
    "print(\"Accuracy:\", acc_score)\n",
    "print(\"Precision:\",prec_score)\n",
    "name.append('knn')\n",
    "precision.append(prec_score)\n",
    "accuracy.append(acc_score)\n",
    "print(name, accuracy, precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier() # n_estimators=100)\n",
    "param_grid = {'n_estimators': np.arange(50,100,5)} \n",
    "rfc_cv = GridSearchCV(rfc, param_grid, cv=5) \n",
    "\n",
    "rfc_cv.fit(X,y)\n",
    "y_pred = rfc_cv.predict(X_2019)\n",
    "y_prob = rfc_cv.predict_proba(X_2019)\n",
    "DFX_2019.loc[:,'predicted_probability_rfc'] = y_prob[:,1]\n",
    "x = DFX_2019.sort_values(by='predicted_probability_rfc', ascending=False).head(10)\n",
    "\n",
    "### Performance evaluation\n",
    "rfc_cv.fit(X_train, y_train)\n",
    "y_pred = rfc_cv.predict(X_test)\n",
    "target_names = ['class 0', 'class 1']\n",
    "clear_output() \n",
    "x = x.iloc[::-1,:]\n",
    "print(rfc_cv.best_params_)\n",
    "plt.barh(x['title'], x['predicted_probability_rfc'])\n",
    "plt.xlabel('probability')\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "acc_score = rfc_cv.score(X, y)\n",
    "prec_score = precision_score(y_test, y_pred, labels= 'class 1')\n",
    "print(\"Accuracy:\", acc_score)\n",
    "print(\"Precision:\",prec_score)\n",
    "name.append('rfc')\n",
    "precision.append(prec_score)\n",
    "accuracy.append(acc_score)\n",
    "print(name, accuracy, precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods: Bag of KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=10), n_estimators=100, random_state=42)\n",
    "bag.fit(X, y)\n",
    "y_pred = bag.predict(X_2019)\n",
    "y_prob = bag.predict_proba(X_2019)\n",
    "DFX_2019.loc[:,'predicted_probability_bag'] = y_prob[:,1]\n",
    "clear_output()\n",
    "x = DFX_2019.sort_values(by='predicted_probability_bag', ascending=False).head(10)\n",
    "\n",
    "\n",
    "### Performance evaluation\n",
    "bag.fit(X_train, y_train)\n",
    "y_pred = bag.predict(X_test)\n",
    "target_names = ['class 0', 'class 1']\n",
    "clear_output() \n",
    "x = x.iloc[::-1,:]\n",
    "# print(bag.best_params_)\n",
    "plt.barh(x['title'], x['predicted_probability_bag'])\n",
    "plt.xlabel('probability')\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "acc_score = bag.score(X, y)\n",
    "prec_score = precision_score(y_test, y_pred, labels= 'class 1')\n",
    "print(\"Accuracy:\", acc_score)\n",
    "print(\"Precision:\",prec_score)\n",
    "name.append('bag')\n",
    "precision.append(prec_score)\n",
    "accuracy.append(acc_score)\n",
    "print(name, accuracy, precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Boosting (Adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=100, random_state=47)\n",
    "ada.fit(X, y)\n",
    "y_pred = ada.predict(X_2019)\n",
    "y_prob = ada.predict_proba(X_2019)\n",
    "\n",
    "DFX_2019.loc[:,'predicted_probability_ada'] = y_prob[:,1]\n",
    "clear_output()\n",
    "print(ada.score(X,y))\n",
    "x = DFX_2019.sort_values(by='predicted_probability_ada', ascending=False).head(10)\n",
    "# print(bag.best_params_)\n",
    "x = x.iloc[::-1,:]\n",
    "\n",
    "\n",
    "### Performance evaluation\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "target_names = ['class 0', 'class 1']\n",
    "clear_output() \n",
    "# print(ada.best_params_)\n",
    "plt.barh(x['title'], x['predicted_probability_ada'])\n",
    "plt.xlabel('probability')\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "acc_score = ada.score(X, y)\n",
    "prec_score = precision_score(y_test, y_pred, labels= 'class 1')\n",
    "print(\"Accuracy:\", acc_score)\n",
    "print(\"Precision:\",prec_score)\n",
    "name.append('ada')\n",
    "precision.append(prec_score)\n",
    "accuracy.append(acc_score)\n",
    "print(name, accuracy, precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(10,10), max_iter=1000, random_state=41)\n",
    "mlp.fit(X, y)\n",
    "y_pred = mlp.predict(X_2019)\n",
    "y_prob = mlp.predict_proba(X_2019)\n",
    "# print(y_prob)\n",
    "# print(classifier.score)\n",
    "DFX_2019.loc[:,'predicted_probability_mlp'] = y_prob[:,1]\n",
    "clear_output()\n",
    "print(mlp.score(X, y))\n",
    "x = DFX_2019.sort_values(by='predicted_probability_mlp', ascending=False).head(10)\n",
    "x = x.iloc[::-1,:]\n",
    "\n",
    "\n",
    "### Performance evaluation\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "target_names = ['class 0', 'class 1']\n",
    "clear_output() \n",
    "plt.barh(x['title'], x['predicted_probability_mlp'])\n",
    "plt.xlabel('probability')\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "acc_score = mlp.score(X, y)\n",
    "prec_score = precision_score(y_test, y_pred, labels= 'class 1')\n",
    "print(\"Accuracy:\", acc_score)\n",
    "print(\"Precision:\",prec_score)\n",
    "name.append('mlp')\n",
    "precision.append(prec_score)\n",
    "accuracy.append(acc_score)\n",
    "print(name, accuracy, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble of Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='sgd')\n",
    "mlp_bag = BaggingClassifier(base_estimator=mlp, n_estimators=100, random_state=42).fit(X, y)\n",
    "y_pred = mlp_bag.predict(X_2019)\n",
    "y_prob = mlp_bag.predict_proba(X_2019)\n",
    "DFX_2019.loc[:,'predicted_probability_mlpbag'] = y_prob[:,1]\n",
    "x = DFX_2019.sort_values(by='predicted_probability_mlpbag', ascending=False).head(10)\n",
    "x = x.iloc[::-1,:]\n",
    "\n",
    "\n",
    "### Performance evaluation\n",
    "mlp_bag.fit(X_train, y_train)\n",
    "y_pred = mlp_bag.predict(X_test)\n",
    "target_names = ['class 0', 'class 1']\n",
    "clear_output() \n",
    "plt.barh(x['title'], x['predicted_probability_mlpbag'])\n",
    "plt.xlabel('probability')\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "acc_score = mlp_bag.score(X, y)\n",
    "prec_score = precision_score(y_test, y_pred, labels= 'class 1')\n",
    "print(\"Accuracy:\", acc_score)\n",
    "print(\"Precision:\",prec_score)\n",
    "name.append('mlp_bag')\n",
    "precision.append(prec_score)\n",
    "accuracy.append(acc_score)\n",
    "print(name, accuracy, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name)\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.DataFrame({'classifier': name, 'accuracy':accuracy, 'precision':precision})\n",
    "performance_df.set_index('classifier', inplace=True)\n",
    "performance_df\n",
    "pickle.dump(performance_df,open('my_data/performance_df', \"wb\" ))\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grand Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "field = 'accuracy'\n",
    "DFX_2019['final_proba'] = 1\n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_log']*performance_df.loc['logreg',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_knn']*performance_df.loc['knn',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_rfc']*performance_df.loc['rfc',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_bag']*performance_df.loc['bag',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_ada']*performance_df.loc['ada',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_mlp']*performance_df.loc['mlp',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_mlpbag']*performance_df.loc['mlp_bag',field] \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "DFX_2019.columns\n",
    "x = DFX_2019.sort_values(by='final_proba', ascending=False).head(10)\n",
    "# print(bag.best_params_)\n",
    "# x = x.iloc[::-1,:]\n",
    "plt.bar(x['title'], x['final_proba'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('predicted\\nprobability')\n",
    "plt.xticks(rotation=45, horizontalalignment = 'right')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "field = 'precision'\n",
    "DFX_2019['final_proba'] = 1\n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_log']*performance_df.loc['logreg',field] \n",
    "# DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_knn']*performance_df.loc['knn',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_rfc']*performance_df.loc['rfc',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_bag']*performance_df.loc['bag',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_ada']*performance_df.loc['ada',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_mlp']*performance_df.loc['mlp',field] \n",
    "DFX_2019['final_proba'] = DFX_2019['final_proba'] * DFX_2019['predicted_probability_mlpbag']*performance_df.loc['mlp_bag',field] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DFX_2019.columns\n",
    "x = DFX_2019.sort_values(by='final_proba', ascending=False).head(10)\n",
    "# print(bag.best_params_)\n",
    "# x = x.iloc[::-1,:]\n",
    "plt.bar(x['title'], x['final_proba'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('predicted\\nprobability')\n",
    "plt.xticks(rotation=45, horizontalalignment = 'right')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(type(X.values))\n",
    "print(type(np.array(y)))\n",
    "len(y[np.where(y==0)])\n",
    "y[1:10]\n",
    "X.values[3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, input_dim=784))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "X_k = K.constant(X.values)\n",
    "X_2019_k = K.constant(X_2019.values)\n",
    "y_k = K.constant(y)\n",
    "\n",
    "ndim = X.shape[1]\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(36, activation = tf.nn.relu, input_dim=ndim))\n",
    "model.add(tf.keras.layers.Dense(36, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(2, activation = tf.nn.softmax))\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "# model.fit(X, y, epochs = 5)\n",
    "model.fit(X_k, y_k, epochs = 5)\n",
    "y_pred = model.predict([X_2019_k])\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = y_pred\n",
    "DFX_2019.loc[:,'predicted_probability_deep'] = y_prob[:,1]\n",
    "x = DFX_2019.sort_values(by='predicted_probability_deep', ascending=False).head(10)\n",
    "x = x.iloc[::-1,:]\n",
    "\n",
    "## Performance evaluation\n",
    "plt.barh(x['title'], x['predicted_probability_deep'])\n",
    "plt.xlabel('probability')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_bag.predict(X_2019)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
